{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ml-data)=\n",
    "# Prepping data for Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this chapter, we're going to look at some issues around preparing data for machine learning. This chapter is enormously indebted to the [**scikit-learn**](https://scikit-learn.org/) documentation and Chris Albon's [Machine Learning Flashcards](https://machinelearningflashcards.com/).\n",
    "\n",
    "The context here is that some machine learning algorithms are not *scale-free*, ie what units your measurements in really matters and you will get better or worse results depending on whether you have rescaled your data appropriately. One algorithm that benefits from this  is the Support Vector Machine. Scaling and pre-processing can help in different ways, but one key way is by easing convergence (such as with non-penalised logistic regression).\n",
    "\n",
    "In this section, we'll also talk about some pitfalls with pre-processing—namely the risk of information leakage.\n",
    "\n",
    "There are a few different ways to scale data, as we'll see, and, when scaling, you will need to remember to put your data back into the original \"space\" if you want to interpret predictions.\n",
    "\n",
    "Of course, machine learning isn't the only context in which you may want to pre-process your data by rescaling it somehow, and these methods can be used in other scenarios too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "random_state = 42  # We'll use this throughout to make this page reproducible\n",
    "prng = np.random.default_rng(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use(\n",
    "    \"https://github.com/aeturrell/coding-for-economists/raw/main/plot_style.txt\"\n",
    ")\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n",
    "\n",
    "# Set max rows displayed for readability\n",
    "pd.set_option(\"display.max_rows\", 6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation and data leakage\n",
    "\n",
    "Standardisation is a common requirement for many machine learning estimators. These estimators might not be able to work at peak performance if the individual features do not more or less look like standard, normally distributed data: that is, a Gaussian with zero mean and unit variance.\n",
    "\n",
    "In practice, we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation. ie\n",
    "\n",
    "$$\n",
    "x_t \\longrightarrow \\frac{x_t - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "**scikit-learn** provides tools for standardisation. We'll demonstrate, first creating some fake data with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[8, 7, 9, 11, 12, 13, 15, 5, 20, 0, 0.43, 16.7],\n",
    "              [0.1, 0.2, 0.3, 0.6, 0.7, 0.8, 0.9, 0.3, 0.7, 0.88, 0.33, 0.22]]).T\n",
    "print(\"The mean of X is:\")\n",
    "print(X.mean(axis=0).round(3))\n",
    "print(\"The std of X is:\")\n",
    "print(X.std(axis=0).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, everything is an object! We've created a scaler object. It has state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaler.mean_)\n",
    "print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use it as a *function* to scale other data. Well, technically, we're using the `transform` *method*, which is available to scaler objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.transform(X)\n",
    "print(\"The mean of X is:\")\n",
    "print(X_scaled.mean(axis=0).round(3))\n",
    "print(\"The std of X is:\")\n",
    "print(X_scaled.std(axis=0).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we come to an important point: *your scaler should only be created from your training data*. Why? Because mean and standard deviations are *global functions* that take information from the *entire* series. Their definitions are:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{T} \\displaystyle\\sum_t x_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{\\displaystyle\\sum_t(x_t - \\mu)^2}{T}}\n",
    "$$\n",
    "\n",
    "And series get transformed as $x_t \\longrightarrow \\frac{x_t-\\mu}{\\sigma}$.\n",
    "\n",
    "So, if you naively use the mean and std from the entire series you are letting information from the test set into your scaling function, and this could enable (erroneously) higher performance. Instead, we want to get the mean, standard deviation, and other properties only including the training set. Let $t'$ denote the training set, which is exclusive of the test set; this is:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{T'} \\displaystyle\\sum_{t'} x_{t'}\n",
    "$$\n",
    "\n",
    "and so on.\n",
    "\n",
    "How does this map back into code? Typically, you'll be doing steps that look like this:\n",
    "\n",
    "```python\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "... training ...\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "```\n",
    "\n",
    "Not all pre-processing functions have this problem—it's only *global* ones. But they are most pre-processing functions, so you do need to take care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minmax scaling and its variants\n",
    "\n",
    "Another popular scaling approach is to use min-max scaling, usually defined as:\n",
    "\n",
    "$$\n",
    "x \\longrightarrow \\frac{x - \\text{max}(x)}{\\text{max}(x) - \\text{min}(x)} \\in (0, 1)\n",
    "$$\n",
    "\n",
    "Once again, this is a *global* scaling, so you want to be careful to only define max and min using training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_mm = preprocessing.MinMaxScaler()\n",
    "X_minmax_scaled = scaler_mm.fit_transform(X)\n",
    "print(f\"The min is {X_minmax_scaled.min():.3f}\")\n",
    "print(f\"The max is {X_minmax_scaled.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variant on minmax scaling is the [`MaxAbsScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler), which scales each feature by its maximum *absolute* value. This means it works with negative value too: values are mapped across several ranges depending on whether negative OR positive values are present. If only positive values are present, the range is [0, 1]. If only negative values are present, the range is [-1, 0]. If both negative and positive values are present, the range is [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both the `MinMaxScaler()` and `MaxAbsScaler()` are prone to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Scaler\n",
    "\n",
    "So what about the situation where you have some outliers? Of course, if they're truly erroneous you may wish to remove them  before you start working with the data (just remember that the test data may have some too!). The other option is to use `RobustScaler()`. The centering and scaling statistics of `RobustScaler` are based on percentiles and are therefore not influenced by a small number of very large marginal outliers. Consequently, the resulting range of the transformed feature values is larger than for the previous scalers. Note that the outliers are retained.\n",
    "\n",
    "Let's see an example straight out of the excellent **scikit-learn** [documentation](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#plot-all-scaling-minmax-scaler-section) with occupancy and income data for housing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Author:  Raghav RV <rvraghav93@gmail.com>\n",
    "#          Guillaume Lemaitre <g.lemaitre58@gmail.com>\n",
    "#          Thomas Unterthiner\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "\n",
    "dataset = fetch_california_housing()\n",
    "X_full, y_full = dataset.data, dataset.target\n",
    "feature_names = dataset.feature_names\n",
    "\n",
    "feature_mapping = {\n",
    "    \"MedInc\": \"Median income in block\",\n",
    "    \"HouseAge\": \"Median house age in block\",\n",
    "    \"AveRooms\": \"Average number of rooms\",\n",
    "    \"AveBedrms\": \"Average number of bedrooms\",\n",
    "    \"Population\": \"Block population\",\n",
    "    \"AveOccup\": \"Average house occupancy\",\n",
    "    \"Latitude\": \"House block latitude\",\n",
    "    \"Longitude\": \"House block longitude\",\n",
    "}\n",
    "\n",
    "# Take only 2 features to make visualization easier\n",
    "# Feature MedInc has a long tail distribution.\n",
    "# Feature AveOccup has a few but very large outliers.\n",
    "features = [\"MedInc\", \"AveOccup\"]\n",
    "features_idx = [feature_names.index(feature) for feature in features]\n",
    "X = X_full[:, features_idx]\n",
    "distributions = [\n",
    "    (\"Unscaled data\", X),\n",
    "    (\"Data after standard scaling\", StandardScaler().fit_transform(X)),\n",
    "    (\"Data after min-max scaling\", MinMaxScaler().fit_transform(X)),\n",
    "    (\"Data after max-abs scaling\", MaxAbsScaler().fit_transform(X)),\n",
    "    (\n",
    "        \"Data after robust scaling\",\n",
    "        RobustScaler(quantile_range=(25, 75)).fit_transform(X),\n",
    "    ),\n",
    "    (\n",
    "        \"Data after power transformation (Yeo-Johnson)\",\n",
    "        PowerTransformer(method=\"yeo-johnson\").fit_transform(X),\n",
    "    ),\n",
    "    (\n",
    "        \"Data after power transformation (Box-Cox)\",\n",
    "        PowerTransformer(method=\"box-cox\").fit_transform(X),\n",
    "    ),\n",
    "    (\n",
    "        \"Data after quantile transformation (uniform pdf)\",\n",
    "        QuantileTransformer(\n",
    "            output_distribution=\"uniform\", random_state=42\n",
    "        ).fit_transform(X),\n",
    "    ),\n",
    "    (\n",
    "        \"Data after quantile transformation (gaussian pdf)\",\n",
    "        QuantileTransformer(\n",
    "            output_distribution=\"normal\", random_state=42\n",
    "        ).fit_transform(X),\n",
    "    ),\n",
    "    (\"Data after sample-wise L2 normalizing\", Normalizer().fit_transform(X)),\n",
    "]\n",
    "\n",
    "# scale the output between 0 and 1 for the colorbar\n",
    "y = minmax_scale(y_full)\n",
    "\n",
    "# plasma does not exist in matplotlib < 1.5\n",
    "cmap = getattr(cm, \"plasma_r\", cm.hot_r)\n",
    "\n",
    "\n",
    "def create_axes(title, figsize=(12, 6)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # define the axis for the first plot\n",
    "    left, width = 0.1, 0.22\n",
    "    bottom, height = 0.1, 0.7\n",
    "    bottom_h = height + 0.15\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    ax_histx = plt.axes(rect_histx)\n",
    "    ax_histy = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the zoomed-in plot\n",
    "    left = width + left + 0.2\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter_zoom = plt.axes(rect_scatter)\n",
    "    ax_histx_zoom = plt.axes(rect_histx)\n",
    "    ax_histy_zoom = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the colorbar\n",
    "    left, width = width + left + 0.13, 0.01\n",
    "\n",
    "    rect_colorbar = [left, bottom, width, height]\n",
    "    ax_colorbar = plt.axes(rect_colorbar)\n",
    "\n",
    "    return (\n",
    "        (ax_scatter, ax_histy, ax_histx),\n",
    "        (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\n",
    "        ax_colorbar,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_distribution(axes, X, y, hist_nbins=50, title=\"\", x0_label=\"\", x1_label=\"\"):\n",
    "    ax, hist_X1, hist_X0 = axes\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x0_label)\n",
    "    ax.set_ylabel(x1_label)\n",
    "\n",
    "    # The scatter plot\n",
    "    colors = cmap(y)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker=\"o\", s=5, lw=0, c=colors)\n",
    "\n",
    "    # Removing the top and the right spine for aesthetics\n",
    "    # make nice axis layout\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines[\"left\"].set_position((\"outward\", 10))\n",
    "    ax.spines[\"bottom\"].set_position((\"outward\", 10))\n",
    "\n",
    "    # Histogram for axis X1 (feature 5)\n",
    "    hist_X1.set_ylim(ax.get_ylim())\n",
    "    hist_X1.hist(\n",
    "        X[:, 1], bins=hist_nbins, orientation=\"horizontal\", color=\"grey\", ec=\"grey\"\n",
    "    )\n",
    "    hist_X1.axis(\"off\")\n",
    "\n",
    "    # Histogram for axis X0 (feature 0)\n",
    "    hist_X0.set_xlim(ax.get_xlim())\n",
    "    hist_X0.hist(\n",
    "        X[:, 0], bins=hist_nbins, orientation=\"vertical\", color=\"grey\", ec=\"grey\"\n",
    "    )\n",
    "    hist_X0.axis(\"off\")\n",
    "\n",
    "\n",
    "def make_plot(item_idx):\n",
    "    title, X = distributions[item_idx]\n",
    "    ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)\n",
    "    axarr = (ax_zoom_out, ax_zoom_in)\n",
    "    plot_distribution(\n",
    "        axarr[0],\n",
    "        X,\n",
    "        y,\n",
    "        hist_nbins=200,\n",
    "        x0_label=feature_mapping[features[0]],\n",
    "        x1_label=feature_mapping[features[1]],\n",
    "        title=\"Full data\",\n",
    "    )\n",
    "\n",
    "    # zoom-in\n",
    "    zoom_in_percentile_range = (0, 99)\n",
    "    cutoffs_X0 = np.percentile(X[:, 0], zoom_in_percentile_range)\n",
    "    cutoffs_X1 = np.percentile(X[:, 1], zoom_in_percentile_range)\n",
    "\n",
    "    non_outliers_mask = np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) & np.all(\n",
    "        X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1\n",
    "    )\n",
    "    plot_distribution(\n",
    "        axarr[1],\n",
    "        X[non_outliers_mask],\n",
    "        y[non_outliers_mask],\n",
    "        hist_nbins=50,\n",
    "        x0_label=feature_mapping[features[0]],\n",
    "        x1_label=feature_mapping[features[1]],\n",
    "        title=\"Zoom-in\",\n",
    "    )\n",
    "\n",
    "    norm = mpl.colors.Normalize(y_full.min(), y_full.max())\n",
    "    mpl.colorbar.ColorbarBase(\n",
    "        ax_colorbar,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        orientation=\"vertical\",\n",
    "        label=\"Colour mapping for values of y\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "make_plot(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Transformations\n",
    "\n",
    "`PowerTransformer` applies a power transformation to each feature to make the data more Gaussian-like in order to stabilise variance and minimise skewness. Currently the Yeo-Johnson and Box-Cox transforms are supported in **scikit-learn** and the optimal scaling factor is determined via maximum likelihood estimation in both methods (see the [Time Series](time-series) chapter for how to estimate $\\lambda$ yourself). By default, PowerTransformer applies zero-mean, unit variance normalisation. Note that Box-Cox can only be applied to strictly positive data.The  happen to be strictly positive, but if negative values are present the Yeo-Johnson transformed is preferred.\n",
    "\n",
    "#### The Box-Cox Transform\n",
    "\n",
    "This transform actually nests power transforms and logarithms, depending on the value of a parameter that is usually denoted $\\lambda$. The transform is given by\n",
    "\n",
    "$$\n",
    "y_t'  =\n",
    "    \\begin{cases}\n",
    "      \\ln(y_t) & \\text{if $\\lambda=0$};  \\\\\n",
    "      (y_t^\\lambda-1)/\\lambda & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "#### The Yeo-Johnson Transform\n",
    "\n",
    "$$\n",
    "y_i = \n",
    "\\begin{cases}\n",
    "    ((y_i + 1)^{\\lambda} - 1) / \\lambda & \\text{if } \\lambda \\neq 0 \\text{ and } y_i \\geq 0 \\\\\n",
    "    -((-y_i + 1)^{2 - \\lambda} - 1) / (2 - \\lambda) & \\text{if } \\lambda \\neq 2 \\text{ and } y_i < 0 \\\\\n",
    "    \\ln(y_i + 1) & \\text{if } \\lambda = 0 \\\\\n",
    "    -\\ln(-y_i + 1) & \\text{if } \\lambda = 2\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### Comparing transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "\n",
    "N_SAMPLES = 1000\n",
    "FONT_SIZE = 6\n",
    "BINS = 30\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(304)\n",
    "bc = PowerTransformer(method=\"box-cox\")\n",
    "yj = PowerTransformer(method=\"yeo-johnson\")\n",
    "# n_quantiles is set to the training set size rather than the default value\n",
    "# to avoid a warning being raised by this example\n",
    "qt = QuantileTransformer(\n",
    "    n_quantiles=500, output_distribution=\"normal\", random_state=rng\n",
    ")\n",
    "size = (N_SAMPLES, 1)\n",
    "\n",
    "\n",
    "# lognormal distribution\n",
    "X_lognormal = rng.lognormal(size=size)\n",
    "\n",
    "# chi-squared distribution\n",
    "df = 3\n",
    "X_chisq = rng.chisquare(df=df, size=size)\n",
    "\n",
    "# weibull distribution\n",
    "a = 50\n",
    "X_weibull = rng.weibull(a=a, size=size)\n",
    "\n",
    "# gaussian distribution\n",
    "loc = 100\n",
    "X_gaussian = rng.normal(loc=loc, size=size)\n",
    "\n",
    "# uniform distribution\n",
    "X_uniform = rng.uniform(low=0, high=1, size=size)\n",
    "\n",
    "# bimodal distribution\n",
    "loc_a, loc_b = 100, 105\n",
    "X_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)\n",
    "X_bimodal = np.concatenate([X_a, X_b], axis=0)\n",
    "\n",
    "\n",
    "# create plots\n",
    "distributions = [\n",
    "    (\"Lognormal\", X_lognormal),\n",
    "    (\"Chi-squared\", X_chisq),\n",
    "    (\"Weibull\", X_weibull),\n",
    "    (\"Gaussian\", X_gaussian),\n",
    "    (\"Uniform\", X_uniform),\n",
    "    (\"Bimodal\", X_bimodal),\n",
    "]\n",
    "\n",
    "colors = [\"#D81B60\", \"#0188FF\", \"#FFC107\", \"#B7A2FF\", \"#000000\", \"#2EC5AC\"]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=8, ncols=3, figsize=plt.figaspect(3))\n",
    "axes = axes.flatten()\n",
    "axes_idxs = [\n",
    "    (0, 3, 6, 9),\n",
    "    (1, 4, 7, 10),\n",
    "    (2, 5, 8, 11),\n",
    "    (12, 15, 18, 21),\n",
    "    (13, 16, 19, 22),\n",
    "    (14, 17, 20, 23),\n",
    "]\n",
    "axes_list = [(axes[i], axes[j], axes[k], axes[l]) for (i, j, k, l) in axes_idxs]\n",
    "\n",
    "\n",
    "for distribution, color, axes in zip(distributions, colors, axes_list):\n",
    "    name, X = distribution\n",
    "    X_train, X_test = train_test_split(X, test_size=0.5)\n",
    "\n",
    "    # perform power transforms and quantile transform\n",
    "    X_trans_bc = bc.fit(X_train).transform(X_test)\n",
    "    lmbda_bc = round(bc.lambdas_[0], 2)\n",
    "    X_trans_yj = yj.fit(X_train).transform(X_test)\n",
    "    lmbda_yj = round(yj.lambdas_[0], 2)\n",
    "    X_trans_qt = qt.fit(X_train).transform(X_test)\n",
    "\n",
    "    ax_original, ax_bc, ax_yj, ax_qt = axes\n",
    "\n",
    "    ax_original.hist(X_train, color=color, bins=BINS)\n",
    "    ax_original.set_title(name, fontsize=FONT_SIZE)\n",
    "    ax_original.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n",
    "\n",
    "    for ax, X_trans, meth_name, lmbda in zip(\n",
    "        (ax_bc, ax_yj, ax_qt),\n",
    "        (X_trans_bc, X_trans_yj, X_trans_qt),\n",
    "        (\"Box-Cox\", \"Yeo-Johnson\", \"Quantile transform\"),\n",
    "        (lmbda_bc, lmbda_yj, None),\n",
    "    ):\n",
    "        ax.hist(X_trans, color=color, bins=BINS)\n",
    "        title = \"After {}\".format(meth_name)\n",
    "        if lmbda is not None:\n",
    "            title += \"\\n$\\\\lambda$ = {}\".format(lmbda)\n",
    "        ax.set_title(title, fontsize=FONT_SIZE)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n",
    "        ax.set_xlim([-3.5, 3.5])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.8.13 ('codeforecon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "caf5ac9f613b176c5984ad2a1a4525760eb7d898a3291351da4c152dc719ffa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
